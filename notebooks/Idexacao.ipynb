{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV5Em2yPEJFQ",
        "outputId": "e1404ac0-7227-4f54-abdc-fbdfb0e06669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Bibliotecas Necessárias\n",
        "\n",
        "!pip install -q transformers sentence-transformers langchain langchain-community faiss-cpu # faiss-cpu for CPU, faiss-gpu if you have a powerful NVIDIA GPU and want faster indexing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports necessários\n",
        "\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "M78UplsQMumd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ccDkcQlL7Dbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração Inicial\n",
        "\n",
        "diretorio = \"todo_texto_extraido\"\n",
        "\n",
        "diretorio_faiss = \"indice_faiss\"\n",
        "os.makedirs(diretorio_faiss, exist_ok=True)\n",
        "\n",
        "modelo = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "tam_chunk = 1000\n",
        "overlap = 100"
      ],
      "metadata": {
        "id": "Xc7-KGaDM_kP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções Auxiliares\n",
        "\n",
        "def carregar_textos(diretorio):\n",
        "\n",
        "  todos_texto = {}\n",
        "  print(f\"\\nCarregando textos do '{diretorio}'...\")\n",
        "\n",
        "  if not os.path.exists(diretorio):\n",
        "    print(f\"O diretório '{diretorio}' não existe. Garanta que o caminho está correto.\")\n",
        "    return todos_texto\n",
        "\n",
        "  for arquivo in os.listdir(diretorio):\n",
        "    if arquivo.endswith(\".txt\"):\n",
        "      caminho_arquivo = os.path.join(diretorio, arquivo)\n",
        "\n",
        "      try:\n",
        "        with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as f:\n",
        "          texto = f.read()\n",
        "          todos_texto[arquivo] = texto\n",
        "          print(f\"Texto do arquivo '{arquivo}' carregado com sucesso.\")\n",
        "      except Exception as e:\n",
        "        print(f\"Erro ao carregar o arquivo '{arquivo}': {e}\")\n",
        "\n",
        "  if not todos_texto:\n",
        "    print(f\"Sem .txt no '{diretorio}'\")\n",
        "\n",
        "  return todos_texto\n",
        "\n",
        "def chunkar_textos(textos_documentos, tam_chunk, overlap):\n",
        "\n",
        "    divisor_texto = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=tam_chunk,\n",
        "        chunk_overlap=overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "        add_start_index=True,\n",
        "    )\n",
        "\n",
        "    textos_chunks = []\n",
        "    print(\"Arquivos detectados:\", list(textos_documentos.keys()))\n",
        "\n",
        "    print(f\"\\nChunkando textos (chunk_size = {tam_chunk}, chunk_overlap = {overlap})...\")\n",
        "\n",
        "    for arquivo, texto in textos_documentos.items():\n",
        "        print(f\"  DEBUG: Document '{os.path.basename(arquivo)}' length BEFORE chunking: {len(texto)} characters.\")\n",
        "        chunks = divisor_texto.create_documents([texto], metadatas=[{\"Fonte\": os.path.basename(arquivo)}])\n",
        "        textos_chunks.extend(chunks)\n",
        "        print(f\"Documento '{os.path.basename(arquivo)}' dividido em {len(chunks)} chunks.\")\n",
        "\n",
        "    if not textos_chunks:\n",
        "        print(\"Nenhum chunk gerado. Cheque se os arquivos de texto não estão vazios.\")\n",
        "    else:\n",
        "        print(f\"Total de chunks gerados: {len(textos_chunks)}\")\n",
        "        print(\"\\nExemplo de um chunk gerado:\")\n",
        "        print(textos_chunks[0].page_content[:200] + \"...\")\n",
        "        print(f\"Metadados: {textos_chunks[0].metadata}\")\n",
        "    return textos_chunks\n",
        "\n",
        "\n",
        "def criar_indice_faiss(chunks, modelo, indice_caminho):\n",
        "\n",
        "  print(f\"Modelo de embedding: {modelo}...\")\n",
        "\n",
        "  kwargs = {}\n",
        "  if os.environ.get('COLAB_GPU', False):\n",
        "    kwargs['device'] = 'cuda'\n",
        "\n",
        "  else:\n",
        "    kwargs['device'] = 'cpu'\n",
        "\n",
        "  embeddings = HuggingFaceEmbeddings(model_name = modelo,\n",
        "                                     model_kwargs = kwargs)\n",
        "\n",
        "\n",
        "  print(f\"Criando o índice FAISS em '{indice_caminho}'...\")\n",
        "  vetor_sema = FAISS.from_documents(chunks, embeddings)\n",
        "  vetor_sema.save_local(indice_caminho)\n",
        "  print(\"Índice FAISS criado com sucesso!\")"
      ],
      "metadata": {
        "id": "gNVmcuQSQw0Z"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analisar_chunks(textos_chunks):\n",
        "    contagem = Counter([chunk.metadata[\"Fonte\"] for chunk in textos_chunks])\n",
        "    print(\"\\nResumo por documento:\")\n",
        "    for nome, count in contagem.items():\n",
        "            print(f\"{nome}: {count} chunks\")\n",
        "    print(f\"Total de chunks contados: {sum(contagem.values())}\")\n",
        "    for i, chunk in enumerate(textos_chunkados):\n",
        "      if chunk.metadata[\"Fonte\"] == \"CÓDIGO DE OBRAS.txt\":\n",
        "        print(\"Chunks do CÓDIGO DE OBRAS.txt:\\n\")\n",
        "        print(f\"Chunk {i}: {len(chunk.page_content)} caracteres\")\n",
        "      if chunk.metadata[\"Fonte\"] == \"tabela.txt\":\n",
        "        print(\"Chunks do tabela.txt:\\n\")\n",
        "        print(f\"Chunk {i}: {len(chunk.page_content)} caracteres\")"
      ],
      "metadata": {
        "id": "vuOrxB8y9tVy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os textos extraidos\n",
        "textos_extraidos = carregar_textos(diretorio)\n",
        "\n",
        "#Chunka os textos\n",
        "textos_chunkados = chunkar_textos(textos_extraidos, tam_chunk, overlap)\n",
        "# analisar_chunks(textos_chunkados)\n",
        "\n",
        "# Cria o índice FAISS\n",
        "banco_vetores = criar_indice_faiss(textos_chunkados, modelo, diretorio_faiss)\n",
        "\n",
        "print(\"\\nIndexão completa!\")\n",
        "print(f\"Os indices estão no diretório: '{diretorio_faiss}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeghF1inY7Ws",
        "outputId": "fb39608a-b8ba-4892-d68c-cd5c1ddc3fc9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Carregando textos do 'todo_texto_extraido'...\n",
            "Texto do arquivo 'CÓDIGO DE OBRAS.txt' carregado com sucesso.\n",
            "Texto do arquivo 'tabela.txt' carregado com sucesso.\n",
            "Arquivos detectados: ['CÓDIGO DE OBRAS.txt', 'tabela.txt']\n",
            "\n",
            "Chunkando textos (chunk_size = 1000, chunk_overlap = 100)...\n",
            "  DEBUG: Document 'CÓDIGO DE OBRAS.txt' length BEFORE chunking: 204239 characters.\n",
            "Documento 'CÓDIGO DE OBRAS.txt' dividido em 226 chunks.\n",
            "  DEBUG: Document 'tabela.txt' length BEFORE chunking: 2500 characters.\n",
            "Documento 'tabela.txt' dividido em 5 chunks.\n",
            "Total de chunks gerados: 231\n",
            "\n",
            "Exemplo de um chunk gerado:\n",
            "PREFEITURA MUNICIPAL DE EUSÉBIO\n",
            "Av. Edimilson Pinheiro n.º 150- Autódromo-CE-CEP61.760-000\n",
            "“PREFEITO E POVO JUNTOS”\n",
            "PLANO DIRETOR DE DESENVOLVIMENTO URBANO DE EUSÉBIO -\n",
            "CÓDIGO DE OBRAS, EDIFICAÇÕES E ...\n",
            "Metadados: {'Fonte': 'CÓDIGO DE OBRAS.txt', 'start_index': 0}\n",
            "Modelo de embedding: sentence-transformers/all-MiniLM-L6-v2...\n",
            "Criando o índice FAISS em 'indice_faiss'...\n",
            "Índice FAISS criado com sucesso!\n",
            "\n",
            "Indexão completa!\n",
            "Os indices estão no diretório: 'indice_faiss'\n"
          ]
        }
      ]
    }
  ]
}